---
title: "Replication Assessment GPID"
author: "Diana C. Garcia Rojas"
date: today
format: 
  html: 
    theme: flatly
    backgroundcolor: snow
    code-fold: true
    footnotes-hover: true
    code-summary: "Examine the code"
    toc: true
    embed-resources: true
    link-external-newwindow: true
execute:
  cache: true
---

## Introduction

This document intends to replicate the assignment developed by the GPID team at the World Bank described in <a href="https://randrescastaneda.github.io/Rtest1/Rtest1.html">R skills assessment</a>. As presented below, I was able to recreate all outputs, tables and graphs. However, the results do not necessarily match the outputs expected by the GPID team. My believe is that, in some cases this is due to the aggregation method used (especially when the result differs only by a small scale), and in other cases it is due to a potential misinterpretation of the population weights. The document has the same structure as the assignment.  

## Basic Stats

### 1. Summary statistics of GDP per capita by region

```{r data}
#| code-summary: "Given code"

tag      <- "202311081903"
base_url <- "https://github.com/randrescastaneda/pub_data/raw/"
data_url <- paste0(base_url, tag, "/data/Rtest1/")


wdi <-
  readr::read_rds(paste0(data_url, "wdi_in1.Rds"))
```

After downloading the data, I recreated the table with statistics of GDP per capita by region and year:

```{r stats}
#| code-summary: "My code"
#| warning: false

# Packages
library(DT)
library(dplyr)
library(Hmisc)

# Check missing values
# sapply(wdi, function(x) sum(is.na(x))) 

# Table grouped by region and date
table <- wdi |>
  select(region,date,gdp, pop)|>
  filter(is.na(gdp)!=TRUE)|>
  rename(year=date)|>
  mutate(gdp_wgt= gdp*pop)

# Stats table with N, mean, sd, min and max
stats <- table |>
  group_by(region, year)|>
  summarise(N=n(),Mean=weighted.mean(gdp,pop),SD=sqrt(Hmisc::wtd.var(gdp,pop)),Min=min(gdp),Max=max(gdp), .groups = "drop")

# Present table                                                             
stats_pre<- rapply(stats, f = round, classes = "numeric", how = "replace", digits = 0)
datatable(stats_pre,options = list(pageLength = 10))
```
<details>
    <summary> Click to see replication test </summary>

There is a difference on the $10^{10}$ decimal point. I used dplyr but I see the final output from the GPID team is in data.table format. It is possible that mean and standard deviations are calculated differently in each format. 

```{r rep_stats}
#| warning: false

# Packages:
library(waldo)

# Data to replicate
stats_given= readr::read_rds(paste0(data_url, "wdi_summ_out.Rds"))

# Comparison
waldo::compare(stats,stats_given)
```

</details>

### 2. Aggregate stats

Here I aggregated the life expectancy (`lifeex`), GDP (`gdp`) , and poverty headcount at international poverty line (`pov_intl`) as requested: 

```{r aggr}
#| warning: false
#| code-summary: "My code"

library(tidyr)
library(reshape2)
library(DescTools)

table_agg <- wdi |>
  select(region, date, lifeex, gdp, pov_intl, pop)|>
  gather("variable", "value", -c(region,date,pop))|>
  group_by(region,date,variable)|>
  summarise(pop_tot= sum(pop),mean= sum(value*pop, na.rm = TRUE)/sum(pop),sd=sqrt(Hmisc::wtd.var(value,pop, na.rm = TRUE)), min = min(value, na.rm = TRUE), max = max(value, na.rm = TRUE), median= DescTools::Median(value, weights =pop, na.rm = TRUE))|>
  rename(pop=pop_tot)|>
  gather("estimate", "value", -c(region,date,pop,variable))|>
  dcast(estimate+region+date+pop~variable)

table_agg_pre<- rapply(table_agg, f = round, classes = "numeric", how = "replace", digits = 3)
datatable(table_agg_pre,options = list(pageLength = 10))
```

<details>
    <summary> Click to see replication test </summary>

Again, there is a difference on the $10^{10}$ decimal point.

```{r rep_aggr}
#| warning: false

# Data to replicate
aggre_given= readr::read_rds(paste0(data_url, "wdi_agg_out.Rds"))

# Order table as wdi_agg_out
table_agg_ord = table_agg|>
  relocate(lifeex, .before = gdp)|>
  arrange(estimate,region, date)

aggre_given_ord =aggre_given|>
  arrange(estimate,region, date)

# Comparison
waldo::compare(table_agg_ord,aggre_given_ord)
```

</details>

### 3. Find outliers

The outliers of `lifeex`, `gpd`, and `gini` by year are the following:

```{r outliers}
#| warning: false
#| code-summary: "My code"

table_out<- wdi |>
  group_by(date)|>
  mutate(across(c(lifeex, gdp, gini), list(mean=~weighted.mean(.,pop,na.rm=TRUE), sd=~sqrt(Hmisc::wtd.var(.,pop,na.rm=TRUE)))))|> 
  ungroup()|>
  mutate(hl_lifeex=(lifeex>(lifeex_mean+(2.5*lifeex_sd))), ll_lifeex=(lifeex<(lifeex_mean-(2.5*lifeex_sd))), hl_gdp=(gdp>(gdp_mean+(2.5*gdp_sd))), ll_gdp=(gdp<(gdp_mean-(2.5*gdp_sd))), hl_gini=(gini>(gini_mean+(2.5*gini_sd))), ll_gini=(gini<(gini_mean-(2.5*gini_sd))))

table_out_pre<- rapply(table_out, f = round, classes = "numeric", how = "replace", digits = 3)
datatable(table_out_pre,options = list(pageLength = 10))
```
<details>
  <summary> Click to see replication test </summary>


```{r rep_out}
#| warning: false

# Given data
out_given=readr::read_rds(paste0(data_url, "wdi_outliers_out.Rds"))

# Order columns

#colnames(table_out)
col_order <- c("region","iso3c","date","country","pov_ofcl", "gdp", "gini","lifeex","pop", "pov_intl", "pov_lmic", "pov_umic","lifeex_mean", "lifeex_sd", "hl_lifeex","ll_lifeex","gdp_mean", "gdp_sd","hl_gdp",  "ll_gdp", "gini_mean", "gini_sd","hl_gini", "ll_gini")

table_out_ord <- table_out[,col_order]

# Comparison
waldo::compare(table_out_ord,out_given)
```

</details>

The chart of `lifeex` of each country in each year, with the weighted mean of `lifeex` per year and the 2.5 confidence interval:

```{r graph_1}
#| warning: false
library(ggplot2)

ggplot(table_out, aes(x=date,y=lifeex))+
  geom_point(aes(col = region))+
  geom_line(aes(y=lifeex_mean))+
  geom_ribbon(aes(ymin=(lifeex_mean-(2.5*lifeex_sd)),ymax=(lifeex_mean+(2.5*lifeex_sd))),alpha=0.3)+
  theme(legend.position = "bottom")
```

## Simulated Data

For the following tasks, there was no link to the data in the assignment. However, I used the data in the repository called `svy_sim_in1.Rds`, because I assumed it was what the GPID team meant by simulated surveys. 

The text in the assignment mentions that *"All the measures should be population-weighted."* and for the following tasks I assumed that the population-weight is the variable in the data.table called `weight`. 

Most of my results in this section, however, do not match perfectly to those presented by the GPID team. I believe this is due to a misinterpretation of the `weight` variable on my part. This is why I do not run the `waldo::compare()` command for the following task as I did for the previous section. 

### 4. Poverty measures

```{r svy_sim}
#| code-summary: "Given code"

# Remove previous data 
rm(wdi)
rm(list=ls(pattern="^table"))
rm(list=ls(pattern="^stats"))
rm(list=ls(pattern="^aggre"))

# Download new data (it is too heavy so I create a new file)
#svy_sim <- readr::read_rds(paste0(data_url, "svy_sim_in1.Rds"))

# Use data
svy_sim <- readRDS(file="svy_sim.Rds")
```

The following are the estimates for poverty headcount, poverty gap and poverty severity for each year using the global poverty lines of \$2.15, \$3.65, and \$6.85 in 2017 PPP prices:

```{r pov_measures}
#| warning: false
#| code-summary: "My code"

# Packages
library(purrr)

# Function FGT
FGT_fun <- function(x,p,d){
  ins = ((p - x[x < p]) / p)^d
  n = length(x)
  FGT = sum(ins)/n
  FGT
}

FGT_fun_w <- function(x,w,p,d){
  ins = ((p - x[x < p])/ p)^d 
  FGT = sum(ins*w[x < p])/sum(w)
  FGT
}


# Add variable of poverty lines
pov_lines <- c(2.15, 3.65, 6.85)
pov_measure= c('headcount', 'povgap','povseverity')

list_pov_m<-list()
for(j in 1:3){
  fgt = pov_measure[j]
  list_pov_m[[fgt]]<-list()
  for (i in 1:length(pov_lines)) {
  pov <- as.character(pov_lines[i])
  list_pov_m[[fgt]][[pov]] <- map(svy_sim,~FGT_fun(.x$income*.x$weight,i,j-1))
}
}

list_pov_w<-list()
for(j in 1:3){
  fgt = pov_measure[j]
  list_pov_w[[fgt]]<-list()
  for (i in 1:length(pov_lines)) {
  pov <- as.character(pov_lines[i])
  list_pov_w[[fgt]][[pov]] <- map(svy_sim,~FGT_fun_w(.x$income,.x$weight,i,j-1))
}
}


# Create data frame and merge
headcount = list_pov_w$headcount |> 
  bind_rows(.id = "pov_line") |>
  pivot_longer(!pov_line, names_to = "Year", values_to = "headcount")

povgap = list_pov_w$povgap |> 
  bind_rows(.id = "pov_line") |>
  pivot_longer(!pov_line, names_to = "Year", values_to = "povgap")

povseverity= list_pov_w$povseverity |> 
  bind_rows(.id = "pov_line") |>
  pivot_longer(!pov_line, names_to = "Year", values_to = "povseverity")

all_measures= merge(headcount,merge(povgap,povseverity,by=c('Year','pov_line')),by=c('Year','pov_line'))

all_measures$Year=as.numeric(substr(all_measures$Year, 2, 5))

datatable(all_measures,options = list(pageLength = 10))
```

Unfortunately, the table found does not match the table given. Also, in the following graph, we can see that headcount does not match especially for 2010. I believe this is the result of a misinterpretation or misuse of the `weight`variable in the calculations.  

```{r graph_pv}
#| warning: false

ggplot(all_measures)+
  geom_line(aes(x=Year,y=headcount, color=pov_line))+
  geom_point(aes(x=Year,y=headcount, color=pov_line))
```

### 5. Lorenz curve

The following code presents the R function I used to create the variables on the Lorenz curve table. Here, as well, there seems to be a mishandling of the weights on my part, because the data does not match the one given by the GPID team.

```{r lorenz}
#| warning: false
#| code-summary: "My code"
#| code-fold: false

# Make it data frame
all_svy <- svy_sim |> 
  bind_rows(.id = "Year") |>
  mutate(Year=as.numeric(substr(Year, 2, 5)))

# Create table
bins= c(1:100)
table_val <- 0

for (i in bins){
  decile <- all_svy |>
  group_by(Year)|>
  summarise(bin=i, 
            welfare=quantile(income,i/100),
            cum_welfare=sum(income[income<=welfare]*weight[income<=welfare])/sum(income*weight),
            cum_population=sum(weight[income<=welfare])/sum(weight))
  
  table_val <- rbind(table_val,decile)
}

table_val= table_val[-1,]
datatable(table_val,options = list(pageLength = 10))
```

However, we can see that the graph shows a good approximation of the Lorenz curve for this population. 

```{r graph2}
#| warning: false

#Graph 

ggplot(table_val)+
  geom_line(aes(x=cum_population,y=cum_welfare,color=as.factor(Year)))

```

### 6. Gini coefficient

Finally, in the following code I created a function to calculate the Gini coefficient. I used a package to calculate the area under the curve, which I hope was allowed.

```{r gini}
#| warning: false

library(smplot2)
gini_tab <- table_val |>
  group_by(Year) |>
  summarise(area=smplot2::sm_auc(cum_population, cum_welfare),gini= 1-2*area)

datatable(gini_tab[,c("Year","gini")],options = list(pageLength = 10))
```

The graph matches closely to the results presented by the GPID team:

```{r graph3}
#| warning: false

ggplot(gini_tab)+
  geom_point(aes(x=Year,y=gini))+
  geom_line(aes(x=Year,y=gini))

```
